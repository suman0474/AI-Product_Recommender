# solution_N/flash_personality.py
# =============================================================================
# FLASH PERSONALITY ARCHITECTURE
# =============================================================================
#
# The "Flash" Architecture: Planning, Executing, Orchestrating
#
# Three-phase personality-aware orchestration:
# 1. PLANNER - Analyzes input and decides strategy (depth, tone, agents)
# 2. EXECUTOR - Runs sub-agents according to plan
# 3. ORCHESTRATOR - Composes final response with personality
#
# =============================================================================

import logging
import os
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from enum import Enum

from common.services.llm.fallback import create_llm_with_fallback
from common.config import AgenticConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

logger = logging.getLogger(__name__)


# =============================================================================
# EXECUTION STRATEGY
# =============================================================================

class ExecutionStrategy(Enum):
    """How deeply to process the request."""
    FAST = "fast"  # Quick response, minimal enrichment
    FULL = "full"  # Complete analysis + enrichment
    DEEP = "deep"  # Deep analysis with standards + learning


class ResponseTone(Enum):
    """Personality tone for response composition."""
    PROFESSIONAL = "professional"  # Formal, technical
    CONVERSATIONAL = "conversational"  # Friendly, approachable
    TECHNICAL = "technical"  # Deep technical detail


@dataclass
class ExecutionPlan:
    """Plan generated by the Flash Planner."""
    strategy: ExecutionStrategy
    tone: ResponseTone
    phases_to_run: List[str]  # Which phases to execute
    skip_enrichment: bool  # Whether to skip standards enrichment
    parallel_identification: bool  # Whether to run ID in parallel
    max_enrichment_items: int  # Max items to enrich (for speed)
    context_depth: str  # "shallow", "moderate", "deep"
    confidence: float  # Planner confidence in this plan
    reasoning: str  # Why this plan was chosen


# =============================================================================
# FLASH PLANNER
# =============================================================================

# Planning prompt
_PLANNING_PROMPT = """You are the Flash Planner for EnGenie's Solution Deep Agent.

Analyze the user's input and conversation context to decide the optimal execution strategy.

USER INPUT: {user_input}

CONVERSATION CONTEXT: {conversation_context}

PERSONAL CONTEXT: {personal_context}

Decide:
1. **strategy**: "fast" (simple request, <3 items), "full" (standard complexity), or "deep" (complex system, safety-critical)
2. **tone**: "professional" (formal/enterprise), "conversational" (casual/exploratory), or "technical" (expert user)
3. **skip_enrichment**: true if user just wants a quick list, false for detailed specs
4. **context_depth**: "shallow" (no prior context needed), "moderate" (some history), "deep" (full conversation analysis)

Indicators for each strategy:
- FAST: "quick list", "just need", simple product names, few items expected
- FULL: Standard solution descriptions, moderate complexity
- DEEP: Safety-critical (SIL, ATEX), complex systems (refinery, reactor), regulatory requirements

OUTPUT (JSON only):
{{
    "strategy": "fast|full|deep",
    "tone": "professional|conversational|technical",
    "skip_enrichment": true|false,
    "context_depth": "shallow|moderate|deep",
    "estimated_items": <number>,
    "reasoning": "<brief explanation>"
}}"""


class FlashPlanner:
    """
    Phase 1: Analyze input and create execution plan.

    Uses a fast LLM call to determine the optimal strategy
    based on input complexity, conversation context, and user profile.
    """

    def plan(
        self,
        user_input: str,
        conversation_context: str = "",
        personal_context: Dict[str, Any] = None,
    ) -> ExecutionPlan:
        """
        Create an execution plan for the solution deep agent.

        Args:
            user_input: The user's input text
            conversation_context: Enriched context from conversation history
            personal_context: User preferences and profile

        Returns:
            ExecutionPlan with strategy, tone, and phase configuration
        """
        personal_context = personal_context or {}

        # Try LLM-based planning
        try:
            llm = create_llm_with_fallback(
                model=AgenticConfig.FLASH_MODEL,
                temperature=0.1,
                google_api_key=os.getenv("GOOGLE_API_KEY"),
                timeout=30,  # Fast timeout for planning
            )

            prompt = ChatPromptTemplate.from_template(_PLANNING_PROMPT)
            parser = JsonOutputParser()
            chain = prompt | llm | parser

            result = chain.invoke({
                "user_input": user_input,
                "conversation_context": conversation_context or "No prior context",
                "personal_context": str(personal_context) if personal_context else "No personal context",
            })

            strategy = ExecutionStrategy(result.get("strategy", "full"))
            tone = ResponseTone(result.get("tone", "professional"))

            # Build phase list based on strategy
            phases = ["classify_intent", "load_context", "analyze_solution", "identify_items"]
            if not result.get("skip_enrichment", False):
                phases.append("enrich_specs")
            phases.extend(["generate_samples", "compose_response"])

            plan = ExecutionPlan(
                strategy=strategy,
                tone=tone,
                phases_to_run=phases,
                skip_enrichment=result.get("skip_enrichment", False),
                parallel_identification=True,
                max_enrichment_items=result.get("estimated_items", 10),
                context_depth=result.get("context_depth", "moderate"),
                confidence=0.85,
                reasoning=result.get("reasoning", "LLM-based planning"),
            )

            logger.info(
                f"[FlashPlanner] Strategy={strategy.value}, Tone={tone.value}, "
                f"Phases={len(phases)}, Skip enrichment={plan.skip_enrichment}"
            )

            return plan

        except Exception as e:
            logger.warning(f"[FlashPlanner] LLM planning failed, using defaults: {e}")
            return self._default_plan(user_input)

    def _default_plan(self, user_input: str) -> ExecutionPlan:
        """Create a default execution plan when LLM planning fails."""
        # Heuristic-based defaults
        input_len = len(user_input)
        has_safety = any(kw in user_input.lower() for kw in ["sil", "atex", "safety", "hazardous"])

        if input_len < 100 and not has_safety:
            strategy = ExecutionStrategy.FAST
        elif has_safety or input_len > 500:
            strategy = ExecutionStrategy.DEEP
        else:
            strategy = ExecutionStrategy.FULL

        return ExecutionPlan(
            strategy=strategy,
            tone=ResponseTone.PROFESSIONAL,
            phases_to_run=[
                "classify_intent", "load_context", "analyze_solution",
                "identify_items", "enrich_specs", "generate_samples",
                "compose_response",
            ],
            skip_enrichment=strategy == ExecutionStrategy.FAST,
            parallel_identification=True,
            max_enrichment_items=15,
            context_depth="moderate",
            confidence=0.6,
            reasoning="Heuristic-based default plan",
        )


# =============================================================================
# FLASH RESPONSE COMPOSER
# =============================================================================

# Response composition prompt
_COMPOSITION_PROMPT = """You are Engenie, a friendly and knowledgeable industrial automation assistant.

TONE: {tone}
SOLUTION NAME: {solution_name}
INDUSTRY: {industry}
TOTAL ITEMS: {total_items}

Compose a response presenting the identified items to the user.

RULES:
- {tone_rules}
- Use the item data provided, do NOT invent items
- Include item numbers, categories, quantities
- Mention key specifications briefly
- End with instructions to select an item for product search
- Keep it concise but informative

ITEMS DATA:
{items_summary}

Generate the response text now."""

TONE_RULES = {
    "professional": "Use formal language, structured bullet points, clear headers",
    "conversational": "Use friendly language, emoji sparingly, feel approachable",
    "technical": "Include detailed specs, use technical terminology, be precise",
}


class FlashResponseComposer:
    """
    Phase 3: Compose the final response with personality awareness.

    Uses the execution plan's tone setting to generate an appropriately
    styled response for the user.
    """

    def compose(
        self,
        items: List[Dict[str, Any]],
        solution_name: str,
        solution_analysis: Dict[str, Any],
        tone: ResponseTone,
        total_items: int,
    ) -> str:
        """
        Compose a personality-aware response.

        Args:
            items: All identified items with specs
            solution_name: Name of the solution
            solution_analysis: Solution analysis results
            tone: Response tone from execution plan
            total_items: Total number of items

        Returns:
            Formatted response string
        """
        try:
            llm = create_llm_with_fallback(
                model=AgenticConfig.FLASH_MODEL,
                temperature=0.3,  # Slightly higher for personality
                google_api_key=os.getenv("GOOGLE_API_KEY"),
                timeout=30,
            )

            # Build items summary
            items_summary = []
            for item in items[:20]:  # Limit for prompt size
                summary = (
                    f"#{item.get('number', '?')}. {item.get('name', 'Unknown')} "
                    f"({item.get('category', '')}) - Qty: {item.get('quantity', 1)} - "
                    f"Type: {item.get('type', 'instrument')}"
                )
                specs = item.get("specifications", {})
                if specs and isinstance(specs, dict):
                    top_specs = list(specs.items())[:5]
                    summary += f" - Specs: {', '.join(f'{k}: {v}' for k, v in top_specs)}"
                items_summary.append(summary)

            prompt = ChatPromptTemplate.from_template(_COMPOSITION_PROMPT)
            chain = prompt | llm | StrOutputParser()

            response = chain.invoke({
                "tone": tone.value,
                "tone_rules": TONE_RULES.get(tone.value, TONE_RULES["professional"]),
                "solution_name": solution_name,
                "industry": solution_analysis.get("industry", "Industrial"),
                "total_items": total_items,
                "items_summary": "\n".join(items_summary),
            })

            return response.strip()

        except Exception as e:
            logger.warning(f"[FlashComposer] LLM composition failed: {e}")
            return self._fallback_compose(items, solution_name, total_items)

    @staticmethod
    def _fallback_compose(
        items: List[Dict[str, Any]],
        solution_name: str,
        total_items: int,
    ) -> str:
        """Fallback plain-text composition when LLM fails."""
        lines = [
            f"**{solution_name} - Instrument Requirements**\n",
            f"I've identified **{total_items} items** for your solution:\n",
        ]

        for item in items:
            emoji = "instrument" if item.get("type") == "instrument" else "accessory"
            lines.append(
                f"{item.get('number', '?')}. [{emoji}] **{item.get('name', 'Unknown')}** "
                f"({item.get('category', '')}) - Qty: {item.get('quantity', 1)}"
            )

        lines.append(f"\nReply with an item number (1-{total_items}) to search for products.")
        return "\n".join(lines)


# =============================================================================
# UNIFIED FLASH PERSONALITY
# =============================================================================

class FlashPersonality:
    """
    Unified Flash Architecture combining Planner + Composer.

    Usage:
        flash = FlashPersonality()
        plan = flash.plan(user_input, context, personal_ctx)
        # ... execute workflow phases ...
        response = flash.compose_response(items, solution_name, analysis, plan)
    """

    def __init__(self):
        self.planner = FlashPlanner()
        self.composer = FlashResponseComposer()

    def plan(
        self,
        user_input: str,
        conversation_context: str = "",
        personal_context: Optional[Dict[str, Any]] = None,
    ) -> ExecutionPlan:
        """Create an execution plan."""
        return self.planner.plan(user_input, conversation_context, personal_context)

    def compose_response(
        self,
        items: List[Dict[str, Any]],
        solution_name: str,
        solution_analysis: Dict[str, Any],
        plan: ExecutionPlan,
        total_items: int,
    ) -> str:
        """Compose the final response using the plan's tone."""
        return self.composer.compose(
            items=items,
            solution_name=solution_name,
            solution_analysis=solution_analysis,
            tone=plan.tone,
            total_items=total_items,
        )
