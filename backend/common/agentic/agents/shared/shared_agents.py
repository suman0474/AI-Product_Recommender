"""
Shared Agents Module

Reusable agents for validation, verification, and generation.
Used by Product Info, Standards RAG, Grounded Chat, and other workflows.

Migrated from engenie_workflow.py to enable reuse across workflows
without creating circular dependencies.
"""

import logging
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException
from common.config import AgenticConfig
from common.services.llm.fallback import create_llm_with_fallback
from common.utils.llm_manager import get_cached_llm


# Import Intent Classification Agent for convenience
# (Avoids circular dependency as it doesn't import from shared_agents)
from common.agentic.agents.routing.intent_classifier import (
    IntentClassificationRoutingAgent,
    WorkflowRoutingResult,
    WorkflowTarget
)

# Import DataSource from engenie_chat_intent_agent for convenience
from chat.engenie_chat_intent_agent import DataSource as IntentDataSource
logger = logging.getLogger(__name__)


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

# Canonical implementation lives in common.utils.json_utils
from common.utils.json_utils import extract_json_from_response


# ============================================================================
# OUTPUT MODELS
# ============================================================================

class ChatResponse(BaseModel):
    """Output from ChatAgent"""
    answer: str = Field(description="The grounded answer to the user's question")
    citations: List[Dict[str, str]] = Field(default_factory=list, description="Source citations")
    rag_sources_used: List[str] = Field(default_factory=list, description="RAG sources queried")
    confidence: float = Field(default=0.0, description="Confidence score 0-1")


class ValidationResult(BaseModel):
    """Output from ResponseValidatorAgent"""
    is_valid: bool = Field(description="Whether the response passes validation")
    overall_score: float = Field(default=0.0, description="Overall validation score 0-1")
    relevance_score: float = Field(default=0.0, description="Relevance to question 0-1")
    accuracy_score: float = Field(default=0.0, description="Factual accuracy 0-1")
    grounding_score: float = Field(default=0.0, description="Grounded in context 0-1")
    citation_score: float = Field(default=0.0, description="Proper citations 0-1")
    hallucination_detected: bool = Field(default=False, description="Hallucination detected")
    issues_found: List[str] = Field(default_factory=list, description="Issues to fix")
    suggestions: str = Field(default="", description="Suggestions for improvement")


class WebVerificationResult(BaseModel):
    """Output from WebSearchVerifierAgent"""
    fact_check_score: float = Field(default=0.5, description="Fact-check score 0-1")
    credibility_score: float = Field(default=0.5, description="Source credibility 0-1")
    hallucination_risk: str = Field(default="medium", description="low/medium/high")
    cross_reference_status: str = Field(default="partial", description="verified/partial/unverified")
    reliable_sources: List[str] = Field(default_factory=list, description="Trustworthy URLs")
    warnings: List[str] = Field(default_factory=list, description="Risk indicators")


# ============================================================================
# PROMPTS - Loaded from consolidated prompts_library file
# ============================================================================


from common.prompts import INDEX_RAG_PROMPTS

CHAT_AGENT_PROMPT = INDEX_RAG_PROMPTS["CHAT_AGENT"]

VALIDATOR_PROMPT = """You are Engenie's Quality Assurance Agent expert in response validation and fact-checking.

Evaluate the generated response across 5 quality dimensions:
1. Relevance (0.0-1.0) - Does response directly address the user's question?
2. Accuracy (0.0-1.0) - Are technical facts correct based on context?
3. Grounding (0.0-1.0) - Is response based ONLY on provided context?
4. Citations (0.0-1.0) - Are sources cited with [Source: filename] format?
5. Completeness (0.0-1.0) - Does response answer ALL parts of the question?

HALLUCINATION PATTERNS (flag these):
- Specific technical values NOT in context
- Vendor/model names NOT in context
- Specifications invented ("typically has X feature")
- Citations to non-existent sources

CRITICAL RULES:
- Grounding score = 0.0 if ANY claim is not in context
- Citations score = 0.0 if ANY factual claim lacks [Source: ...] citation
- overall_quality = average of 5 dimension scores
- validation_passed = true only if overall_quality >= 0.8

OUTPUT (JSON only):
{{
  "relevance_score": 0.0-1.0,
  "accuracy_score": 0.0-1.0,
  "grounding_score": 0.0-1.0,
  "citations_score": 0.0-1.0,
  "completeness_score": 0.0-1.0,
  "overall_quality": 0.0-1.0,
  "hallucinations": ["<specific ungrounded claim>"],
  "citation_issues": ["<specific missing or incorrect citation>"],
  "improvement_suggestions": ["<specific actionable suggestion>"],
  "validation_passed": true/false
}}

USER QUESTION: {user_question}
PROVIDED CONTEXT: {context}
GENERATED RESPONSE: {generated_response}"""

WEB_VERIFIER_PROMPT = """You are a web source credibility analyst. Score the provided search result across 4 dimensions:
1. Source Credibility (0.0-1.0) — Tier 1: manufacturer/standards org (0.9-1.0), Tier 2: industry publications (0.7-0.9), Tier 3: general websites (0.4-0.6), Tier 4: unverified (0.0-0.3)
2. Content Accuracy (0.0-1.0) — Are technical facts verifiable and consistent?
3. Relevance (0.0-1.0) — Does content directly address the query?
4. Recency (0.0-1.0) — Is information current?

RED FLAGS: No author/publisher, outdated content (>5 years for technical specs), conflicts with credible sources, marketing masquerading as technical info.

overall_credibility = weighted average: credibility 40%, accuracy 30%, relevance 20%, recency 10%

OUTPUT (JSON only):
{{
  "source_credibility": 0.0-1.0,
  "content_accuracy": 0.0-1.0,
  "relevance": 0.0-1.0,
  "recency": 0.0-1.0,
  "overall_credibility": 0.0-1.0,
  "credibility_tier": "<Tier 1/2/3/4>",
  "red_flags": ["<specific issue>"],
  "recommendation": "use" | "use_with_caution" | "reject",
  "reasoning": "<brief explanation>"
}}

WEB SEARCH RESULT: {search_result}
URL: {url}
QUERY: {query}"""


# ============================================================================
# RESPONSE VALIDATOR AGENT
# ============================================================================

class ResponseValidatorAgent:
    """
    Validates responses for grounding, accuracy, and hallucination.

    5-dimensional validation:
    1. Relevance - Does response address the question?
    2. Accuracy - Is information factually correct?
    3. Grounding - Is response grounded in context?
    4. Citations - Are sources properly cited?
    5. Hallucination - Any fabricated information?
    """

    def __init__(self, llm=None, components: Dict = None):
        self.components = components or {}
        self.llm = llm or get_cached_llm(
            model=AgenticConfig.FLASH_MODEL,
            temperature=0.1  # Low temperature for objective validation
        )
        logger.info("ResponseValidatorAgent initialized")

    def run(
        self,
        response: str,
        user_question: str,
        context: str = ""
    ) -> Dict[str, Any]:
        """Validate a generated response.

        Args:
            response: The generated response to validate
            user_question: The original user question
            context: Available context from sources

        Returns:
            Validation result with scores and issues
        """
        logger.info("ResponseValidatorAgent validating response...")

        try:
            prompt = ChatPromptTemplate.from_template(VALIDATOR_PROMPT)
            parser = JsonOutputParser()
            chain = prompt | self.llm | parser
            
            result = None
            
            # Try using the chain with JsonOutputParser first
            try:
                result = chain.invoke({
                    "user_question": user_question,
                    "generated_response": response,
                    "context": context or "No context provided"
                })
            except (OutputParserException, json.JSONDecodeError) as parse_error:
                # Fallback: invoke LLM directly without parser
                logger.warning(f"Validation parsing failed, attempting fallback: {parse_error}")

                raw_chain = prompt | self.llm
                raw_response = raw_chain.invoke({
                    "user_question": user_question,
                    "generated_response": response,
                    "context": context or "No context provided"
                })
                
                # Extract text content
                if hasattr(raw_response, 'content'):
                    raw_text = raw_response.content
                elif hasattr(raw_response, 'text'):
                    raw_text = raw_response.text
                else:
                    raw_text = str(raw_response)
                
                # Use helper function to extract JSON
                result = extract_json_from_response(raw_text)
                
                if result:
                    logger.info("Successfully extracted validation JSON using fallback")
                else:
                    # Default to assuming valid if we can't parse
                    logger.warning("Could not parse validation result, defaulting to valid=True")
                    result = {
                        "is_valid": True,
                        "overall_score": 0.7,
                        "parsing_fallback": True
                    }

            if result is None:
                result = {}

            is_valid = result.get("is_valid", False)
            overall_score = result.get("overall_score", 0.0)

            logger.info(f"Validation complete: valid={is_valid}, score={overall_score:.2f}")

            return {
                "success": True,
                "is_valid": is_valid,
                "overall_score": overall_score,
                "relevance_score": result.get("relevance_score", 0.0),
                "accuracy_score": result.get("accuracy_score", 0.0),
                "grounding_score": result.get("grounding_score", 0.0),
                "citation_score": result.get("citation_score", 0.0),
                "hallucination_detected": result.get("hallucination_detected", False),
                "issues_found": result.get("issues_found", []),
                "suggestions": result.get("suggestions", "")
            }

        except Exception as e:
            logger.error(f"ResponseValidatorAgent failed: {e}")
            return {
                "success": False,
                "is_valid": True,  # Default to valid to avoid blocking the workflow
                "overall_score": 0.5,
                "issues_found": [f"Validation error: {str(e)}"],
                "error": str(e)
            }


# ============================================================================
# WEB SEARCH VERIFIER AGENT
# ============================================================================

class WebSearchVerifierAgent:
    """
    Verifies web search results for credibility and accuracy.

    Performs 4-dimensional verification:
    1. Fact-checking across sources
    2. Source credibility assessment
    3. Hallucination risk evaluation
    4. Cross-reference status
    """

    def __init__(self, llm=None, components: Dict = None):
        self.components = components or {}
        self.llm = llm or get_cached_llm(
            model=AgenticConfig.FLASH_MODEL,
            temperature=0.1  # Low temperature for objective verification
        )
        logger.info("WebSearchVerifierAgent initialized")

    def run(
        self,
        user_question: str,
        web_results: List[Dict[str, Any]],
        entities: List[str] = None
    ) -> Dict[str, Any]:
        """Verify web search results.

        Args:
            user_question: The user's question
            web_results: List of web search results
            entities: Extracted entities from the question

        Returns:
            Verification result with scores and warnings
        """
        logger.info(f"WebSearchVerifierAgent verifying {len(web_results)} results...")

        try:
            # Format web results for prompt
            results_text = ""
            for i, result in enumerate(web_results[:5], 1):
                title = result.get("title", "No title")
                snippet = result.get("snippet", result.get("description", "No snippet"))
                link = result.get("link", result.get("url", "No link"))
                results_text += f"\n{i}. [{title}]({link})\n   {snippet}\n"

            prompt = ChatPromptTemplate.from_template(WEB_VERIFIER_PROMPT)
            parser = JsonOutputParser()
            chain = prompt | self.llm | parser

            result = chain.invoke({
                "question": user_question,
                "web_results": results_text,
                "entities": ", ".join(entities) if entities else "None specified"
            })

            logger.info(f"Verification: credibility={result.get('credibility_score', 0):.2f}, "
                       f"risk={result.get('hallucination_risk', 'unknown')}")

            return {
                "success": True,
                "fact_check_score": result.get("fact_check_score", 0.5),
                "credibility_score": result.get("credibility_score", 0.5),
                "hallucination_risk": result.get("hallucination_risk", "medium"),
                "cross_reference_status": result.get("cross_reference_status", "partial"),
                "reliable_sources": result.get("reliable_sources", []),
                "warnings": result.get("warnings", [])
            }

        except Exception as e:
            logger.error(f"WebSearchVerifierAgent failed: {e}")
            return {
                "success": False,
                "fact_check_score": 0.5,
                "credibility_score": 0.5,
                "hallucination_risk": "medium",
                "cross_reference_status": "partial",
                "reliable_sources": [],
                "warnings": [f"Verification error: {str(e)}"],
                "error": str(e)
            }


# ============================================================================
# CHAT AGENT (Optional - for generating grounded responses)
# ============================================================================

class ChatAgent:
    """
    Generates grounded responses with citations using RAG context.
    """

    def __init__(self, llm=None, components: Dict = None):
        self.components = components or {}
        self.llm = llm or get_cached_llm(
            model=AgenticConfig.FLASH_MODEL,
            temperature=0.3
        )
        logger.info("ChatAgent initialized")

    def run(
        self,
        user_question: str,
        product_type: str = "",
        rag_context: Dict[str, Any] = None,
        specifications: Dict[str, Any] = None,
        user_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Generate a grounded answer to the user's question.

        Args:
            user_question: The user's question
            product_type: Type of product being queried
            rag_context: RAG context from various sources
            specifications: Company specifications and preferences
            user_context: Additional user context

        Returns:
            Generated response with citations
        """
        logger.info(f"ChatAgent processing: {user_question[:50]}...")

        try:
            rag_context = rag_context or {}
            specifications = specifications or {}

            rag_context_str = json.dumps(rag_context, indent=2) if isinstance(rag_context, dict) else str(rag_context)

            preferred_vendors = specifications.get("preferred_vendors", [])
            required_standards = specifications.get("required_standards", [])
            installed_series = specifications.get("installed_series", [])

            prompt = ChatPromptTemplate.from_template(CHAT_AGENT_PROMPT)
            parser = JsonOutputParser()
            chain = prompt | self.llm | parser

            result = chain.invoke({
                "question": user_question,
                "product_type": product_type or "general",
                "rag_context": rag_context_str,
                "preferred_vendors": ", ".join(preferred_vendors) if preferred_vendors else "Not specified",
                "required_standards": ", ".join(required_standards) if required_standards else "Not specified",
                "installed_series": ", ".join(installed_series) if installed_series else "Not specified"
            })

            return {
                "success": True,
                "answer": result.get("answer", ""),
                "citations": result.get("citations", []),
                "rag_sources_used": result.get("rag_sources_used", []),
                "confidence": result.get("confidence", 0.0)
            }

        except Exception as e:
            logger.error(f"ChatAgent failed: {e}")
            return {
                "success": False,
                "answer": "I apologize, but I encountered an error generating an answer.",
                "citations": [],
                "rag_sources_used": [],
                "confidence": 0.0,
                "error": str(e)
            }


# ============================================================================
# SESSION MANAGER AGENT
# ============================================================================

class SessionManagerAgent:
    """
    Manages multi-turn conversation memory.
    """

    def __init__(self, llm=None, components: Dict = None):
        self.components = components or {}
        self.sessions: Dict[str, Dict] = {}
        logger.info("SessionManagerAgent initialized")

    def run(
        self,
        session_id: str,
        question: str = None,
        answer: str = None,
        validation_score: float = 0.0,
        citations: List[Dict] = None,
        operation: str = "update"
    ) -> Dict[str, Any]:
        """Manage session state.

        Args:
            session_id: Session identifier
            question: User question (for update)
            answer: Generated answer (for update)
            validation_score: Validation score (for update)
            citations: Source citations (for update)
            operation: Operation type - "update", "get", or "clear"

        Returns:
            Session state information
        """
        from datetime import datetime
        logger.info(f"SessionManagerAgent: {operation} for session {session_id}")

        try:
            if operation == "clear":
                if session_id in self.sessions:
                    del self.sessions[session_id]
                return {
                    "success": True,
                    "session_id": session_id,
                    "total_interactions": 0,
                    "updated": True,
                    "message": "Session cleared"
                }

            if operation == "get":
                session = self.sessions.get(session_id, {})
                return {
                    "success": True,
                    "session_id": session_id,
                    "total_interactions": len(session.get("interactions", [])),
                    "interactions": session.get("interactions", []),
                    "updated": False
                }

            # Update operation
            if session_id not in self.sessions:
                self.sessions[session_id] = {
                    "created_at": datetime.utcnow().isoformat(),
                    "interactions": []
                }

            session = self.sessions[session_id]

            if question and answer:
                interaction = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "question": question,
                    "answer": answer,
                    "validation_score": validation_score,
                    "citations": citations or []
                }
                session["interactions"].append(interaction)
                session["last_updated"] = datetime.utcnow().isoformat()

            total_interactions = len(session["interactions"])

            logger.info(f"Session {session_id}: {total_interactions} interactions")

            return {
                "success": True,
                "session_id": session_id,
                "total_interactions": total_interactions,
                "updated": True
            }

        except Exception as e:
            logger.error(f"SessionManagerAgent failed: {e}")
            return {
                "success": False,
                "session_id": session_id,
                "error": str(e)
            }

    def get_conversation_context(self, session_id: str, max_turns: int = 5) -> str:
        """Get formatted conversation history for context.

        Args:
            session_id: Session identifier
            max_turns: Maximum number of turns to include

        Returns:
            Formatted conversation context string
        """
        session = self.sessions.get(session_id, {})
        interactions = session.get("interactions", [])

        if not interactions:
            return ""

        recent = interactions[-max_turns:]

        context_parts = ["Previous conversation:"]
        for i, interaction in enumerate(recent, 1):
            context_parts.append(f"Q{i}: {interaction['question']}")
            context_parts.append(f"A{i}: {interaction['answer'][:200]}...")

        return "\n".join(context_parts)


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def optimize_web_query(
    user_question: str,
    entities: List[str]
) -> str:
    """
    Optimize user question for web search.

    - Remove question words
    - Add key entities
    - Add industrial context if needed

    Args:
        user_question: Original user question
        entities: Extracted entities from the question

    Returns:
        Optimized search query
    """
    query = user_question.lower()

    # Remove common question words
    question_words = ["what is", "how does", "why", "explain", "tell me about", "can you"]
    for qw in question_words:
        query = query.replace(qw, "")

    # Add entities
    if entities:
        query = f"{query} {' '.join(entities[:3])}"

    # Clean up
    query = query.strip()

    # Add industrial context if not already present
    industrial_keywords = ["industrial", "instrumentation", "transmitter", "sensor", "valve", "automation"]
    if not any(kw in query.lower() for kw in industrial_keywords):
        query = f"{query} industrial instrumentation"

    return query


# ============================================================================
# EXPORTS
# ============================================================================

__all__ = [
    # Models
    "ChatResponse",
    "ValidationResult",
    "WebVerificationResult",
    "WorkflowRoutingResult",
    # Agents
    "ResponseValidatorAgent",
    "WebSearchVerifierAgent",
    "ChatAgent",
    "SessionManagerAgent",
    "IntentClassificationRoutingAgent",
    # Prompts
    "CHAT_AGENT_PROMPT",
    "VALIDATOR_PROMPT",
    "WEB_VERIFIER_PROMPT",
    # Functions
    "optimize_web_query",
    "extract_json_from_response",
    # Intent Classification (re-exported for convenience)
    "IntentDataSource"  # Aliased from DataSource to avoid naming conflicts
]
